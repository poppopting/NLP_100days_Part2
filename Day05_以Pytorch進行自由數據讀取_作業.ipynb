{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的: 熟練自定義collate_fn與sampler進行資料讀取\n",
    "\n",
    "本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n",
    "客製化資料讀取。\n",
    "下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n",
    "(請同學先行至IMDB下載資料)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Guan-Ting\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Guan-Ting\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import torch and other required modules\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #下載stopwords\n",
    "nltk.download('punkt') #下載word_tokenize需要的corpus\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索資料與資料前處理\n",
    "這份作業我們使用test資料中的pos與neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length before removing stopwords: 89527\n",
      "vocab length after removing stopwords: 89356\n"
     ]
    }
   ],
   "source": [
    "# 讀取字典，這份字典為review內所有出現的字詞\n",
    "with open('aclImdb/imdb.vocab', 'r', encoding='utf-8') as f:\n",
    "    vocab = f.read().split('\\n')\n",
    "\n",
    "# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n",
    "print(f\"vocab length before removing stopwords: {len(vocab)}\")\n",
    "vocab = [word for word in vocab if word not in stopwords]\n",
    "print(f\"vocab length after removing stopwords: {len(vocab)}\")\n",
    "\n",
    "# 將字典轉換成dictionary\n",
    "vocab_dic = {}\n",
    "idx = 0\n",
    "for word in vocab:\n",
    "    if word not in vocab_dic:\n",
    "        vocab_dic[word] = idx\n",
    "        idx += 1\n",
    "        \n",
    "# vocab_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('./aclImdb/train/pos\\\\0_9.txt', 1), ('./aclImdb/train/pos\\\\10000_8.txt', 1)]\n",
      "Total reviews: 25000\n"
     ]
    }
   ],
   "source": [
    "# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n",
    "# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n",
    "# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n",
    "review_pairs = []\n",
    "for tag in ['pos', 'neg']:\n",
    "    label = int(tag=='pos')\n",
    "    for file in glob.glob(f'./aclImdb/train/{tag}/*.txt'):\n",
    "        review_pairs.append((file, label))\n",
    "\n",
    "print(review_pairs[:2])\n",
    "print(f\"Total reviews: {len(review_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立Dataset, DataLoader, Sampler與Collate_fn讀取資料\n",
    "這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量函式\n",
    "(generate_vec)，注意這裡我們用來產生詞向量的方法是單純將文字tokenize(為了使產生的文本長度不同，而不使用BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review(review_path):\n",
    "    \n",
    "    with open(review_path, 'r', encoding='utf-8') as f:\n",
    "        review = f.read()\n",
    "        \n",
    "    #移除non-alphabet符號、贅字與tokenize\n",
    "    review = re.sub('\\W', ' ', review)\n",
    "    review = nltk.word_tokenize(review)\n",
    "    review = [word for word in review if word not in stopwords]\n",
    "    return review\n",
    "\n",
    "def generate_vec(review, vocab_dic):\n",
    "    review_vec = [vocab_dic[word] for word in review if vocab_dic.get(word)]\n",
    "    return np.asarray(review_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立客製化dataset\n",
    "\n",
    "class dataset(Dataset):\n",
    "    '''custom dataset to load reviews and labels\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_pairs: list\n",
    "        directory of all review-label pairs\n",
    "    vocab: list\n",
    "        list of vocabularies\n",
    "    '''\n",
    "    def __init__(self, data_dirs, vocab):\n",
    "        self.data_dirs = data_dirs\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review_path, tag = self.data_dirs[idx]\n",
    "        review = load_review(review_path)\n",
    "        vector = generate_vec(review, self.vocab)\n",
    "        return (vector, tag)\n",
    "    \n",
    "\n",
    "#建立客製化collate_fn，將長度不一的文本pad 0 變成相同長度\n",
    "def collate_fn(batch):\n",
    "\n",
    "    reviews, labels = zip(*batch) \n",
    "    \n",
    "    ### create pads for reviews ###\n",
    "    lengths = [len(x) for x in reviews]\n",
    "    max_length = max(lengths)\n",
    "    \n",
    "    batch_reviews = []\n",
    "    \n",
    "    for i in range(len(reviews)):\n",
    "        # pad reviews\n",
    "        tmp_pads = torch.zeros(max_length)\n",
    "        tmp_pads[:lengths[i]] = torch.from_numpy(reviews[i])\n",
    "        batch_reviews.append(tmp_pads.view(1,-1))\n",
    "\n",
    "    return torch.cat(batch_reviews,dim=0), torch.tensor(labels) , torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 使用Pytorch的RandomSampler來進行indice讀取並建立dataloader\n",
    "custom_dst = dataset(review_pairs, vocab_dic)\n",
    "custom_dataloader = DataLoader(custom_dst, batch_size=4, sampler=RandomSampler(custom_dst), collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[        7.,       307.,       635.,       320.,       326.,       779.,\n",
       "               4240.,     11373.,      1112.,       212.,      1982.,     22826.,\n",
       "                374.,         6.,      1969.,      1814.,       276.,       128.,\n",
       "               6697.,      3638.,        28.,        17.,        28.,     15079.,\n",
       "              11277.,       212.,       476.,        28.,     15079.,      2126.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.],\n",
       "         [     4585.,       481.,       113.,       930.,        93.,       873.,\n",
       "               2796.,      1124.,       449.,       276.,       436.,      2217.,\n",
       "               1759.,        37.,         8.,      1963.,        10.,        15.,\n",
       "               3085.,       122.,       446.,       575.,       102.,       329.,\n",
       "                438.,        39.,       194.,       102.,       373.,      1524.,\n",
       "               1805.,        13.,       420.,       453.,       150.,        30.,\n",
       "                267.,      9772.,        27.,      5732.,      3349.,       336.,\n",
       "               1009.,       831.,        22.,        17.,        10.,       169.,\n",
       "                  2.,     12075.,       511.,        52.,        15.,       511.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.],\n",
       "         [     5820.,       290.,       532.,       323.,        42.,         2.,\n",
       "                  1.,        33.,      2252.,      1606.,        75.,       755.,\n",
       "               6628.,         1.,       307.,       971.,        22.,       307.,\n",
       "                  1.,       234.,        22.,        25.,        43.,      1134.,\n",
       "                129.,       469.,        16.,        48.,         1.,      1036.,\n",
       "               1440.,        11.,        61.,        93.,       142.,       110.,\n",
       "                219.,       547.,       141.,        24.,        29.,         1.,\n",
       "                123.,       933.,       215.,       572.,         1.,      2828.,\n",
       "                 85.,       606.,      1557.,       983.,      2234.,      1403.,\n",
       "                156.,        16.,       149.,      1112.,       653.,       971.,\n",
       "                234.,       259.,       971.,       234.,       259.,       971.,\n",
       "                234.,       259.,       384.,       384.,         1.,       853.,\n",
       "                  7.,       234.,       721.,      1112.,     75154.,     75154.,\n",
       "                532.,       393.,         1.,       147.,         9.,       808.,\n",
       "               2753.,        22.,        14.,        68.,       168.,       137.,\n",
       "               2753.,         1.,      7698.,      3820.,       393.,      1847.,\n",
       "                  1.,       815.,       247.,      2933.,        10.,        15.,\n",
       "               1500.,        37.,      5398.,      1557.,        68.,       450.,\n",
       "                110.,         1.,       603.,         4.,       804.,        19.,\n",
       "                 96.,      6759.,      1461.,        63.,      2053.,       903.,\n",
       "              10802.,      1040.,      2166.,       249.,      1112.,         1.,\n",
       "                749.,       206.,      1055.,     16405.,        45.,       513.,\n",
       "                773.,       847.,       273.,       105.,        71.,        82.,\n",
       "               1500.,       194.,       238.,         1.,      2185.,      1479.,\n",
       "                  6.,        81.,        48.,       462.,        26.,       296.,\n",
       "                770.,       508.,       890.,         1.,        33.,      2219.,\n",
       "                 61.,      4503.,        11.,      3102.,        29.,       223.,\n",
       "                608.,         1.,       110.,       141.,      1808.,       393.,\n",
       "                 14.,      3858.,       574.],\n",
       "         [     3582.,       195.,      5898.,        30.,      7220.,      1202.,\n",
       "                546.,      1244.,       323.,        34.,      1767.,       774.,\n",
       "                569.,       123.,        64.,       113.,        36.,      1306.,\n",
       "               5484.,      1019.,     38699.,       368.,       186.,       142.,\n",
       "               7998.,      1808.,       292.,      5422.,        47.,        20.,\n",
       "                580.,       124.,         4.,      7998.,      1808.,      1035.,\n",
       "               6498.,        68.,      1679.,      1250.,       116.,       127.,\n",
       "                  4.,       174.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.,         0.,         0.,         0.,\n",
       "                  0.,         0.,         0.]]),\n",
       " tensor([0, 1, 0, 0]),\n",
       " tensor([ 30,  54, 171,  44]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(custom_dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
