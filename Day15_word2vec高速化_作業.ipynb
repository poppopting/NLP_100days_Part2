{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5Pf_RxOIAYv"
   },
   "source": [
    "### 作業目的: 透過實作加速版word2vec Skip-gram模型來更加了解高速版的word2vec\n",
    "\n",
    "本次作業會採用Penn Tree Bank資料及，學員可以在ptb.train.txt中取得訓練文本資料。這次作業可以讓學員練習到以pytorch搭建模型與進行文本資料的前處理\n",
    "\n",
    "PS: 建議學員使用Colab (或可以使用GPU加速的機器)來進行作業，不然訓練會訓練到天荒地老....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO-a6e2OI5zg"
   },
   "source": [
    "### Connect to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 14937,
     "status": "ok",
     "timestamp": 1606320756664,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "LXPU7BI3HNJ6"
   },
   "outputs": [],
   "source": [
    "# # Import libraries for importing files from Google drive to Colab\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# # Authorize Google SDK to access Google Drive from Colab\n",
    "\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 17072,
     "status": "ok",
     "timestamp": 1606320758810,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "D2E7yb-qI9Uv"
   },
   "outputs": [],
   "source": [
    "# download = drive.CreateFile({'id': '請自行輸入自己上傳google drive檔案的連結id'})\n",
    "# download.GetContentFile('ptb.train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKKpFV6GJwhs"
   },
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4149,
     "status": "ok",
     "timestamp": 1606320764926,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "Yjz-fWmbJRPB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import urllib.request\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3426,
     "status": "ok",
     "timestamp": 1606320764930,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "i9xrgPu3KBgJ",
    "outputId": "341dcbac-256c-45ee-ed8d-3ee0c1fb03b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 42068 lines\n"
     ]
    }
   ],
   "source": [
    "# 讀取資料\n",
    "\n",
    "# Penn Tree Back dataset\n",
    "with open(\"./ptb.train.txt\", encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(f\"Total {len(lines)} lines\")\n",
    "raw_dataset = [line.split() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2988,
     "status": "ok",
     "timestamp": 1606320764931,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "oAcF_5CQKH_J",
    "outputId": "b42ef993-9894-4061-f8df-3f929fe17114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aer',\n",
       "  'banknote',\n",
       "  'berlitz',\n",
       "  'calloway',\n",
       "  'centrust',\n",
       "  'cluett',\n",
       "  'fromstein',\n",
       "  'gitano',\n",
       "  'guterman',\n",
       "  'hydro-quebec',\n",
       "  'ipo',\n",
       "  'kia',\n",
       "  'memotec',\n",
       "  'mlx',\n",
       "  'nahb',\n",
       "  'punts',\n",
       "  'rake',\n",
       "  'regatta',\n",
       "  'rubens',\n",
       "  'sim',\n",
       "  'snack-food',\n",
       "  'ssangyong',\n",
       "  'swapo',\n",
       "  'wachter'],\n",
       " ['pierre',\n",
       "  '<unk>',\n",
       "  'N',\n",
       "  'years',\n",
       "  'old',\n",
       "  'will',\n",
       "  'join',\n",
       "  'the',\n",
       "  'board',\n",
       "  'as',\n",
       "  'a',\n",
       "  'nonexecutive',\n",
       "  'director',\n",
       "  'nov.',\n",
       "  'N'],\n",
       " ['mr.',\n",
       "  '<unk>',\n",
       "  'is',\n",
       "  'chairman',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'n.v.',\n",
       "  'the',\n",
       "  'dutch',\n",
       "  'publishing',\n",
       "  'group'],\n",
       " ['rudolph',\n",
       "  '<unk>',\n",
       "  'N',\n",
       "  'years',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'chairman',\n",
       "  'of',\n",
       "  'consolidated',\n",
       "  'gold',\n",
       "  'fields',\n",
       "  'plc',\n",
       "  'was',\n",
       "  'named',\n",
       "  'a',\n",
       "  'nonexecutive',\n",
       "  'director',\n",
       "  'of',\n",
       "  'this',\n",
       "  'british',\n",
       "  'industrial',\n",
       "  'conglomerate'],\n",
       " ['a',\n",
       "  'form',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'used',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters',\n",
       "  'has',\n",
       "  'caused',\n",
       "  'a',\n",
       "  'high',\n",
       "  'percentage',\n",
       "  'of',\n",
       "  'cancer',\n",
       "  'deaths',\n",
       "  'among',\n",
       "  'a',\n",
       "  'group',\n",
       "  'of',\n",
       "  'workers',\n",
       "  'exposed',\n",
       "  'to',\n",
       "  'it',\n",
       "  'more',\n",
       "  'than',\n",
       "  'N',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'researchers',\n",
       "  'reported']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看前5筆\n",
    "raw_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3502,
     "status": "ok",
     "timestamp": 1606320765980,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "3oki6AxhJyj4",
    "outputId": "79a06d23-2176-4600-b55b-033dfb81ce49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before subsampling: 885720 words\n",
      "After subsampling: 437018 words\n"
     ]
    }
   ],
   "source": [
    "# 定義資料前處理函示\n",
    "class PreProcessor():\n",
    "    '''Function to do preprocess of input corpus\n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: str\n",
    "        input corpus to be processed\n",
    "    only_word: bool\n",
    "        whether to filter out non-word\n",
    "    min_freq: int\n",
    "        minimum frequency of a word to be kept\n",
    "    do_subsampling: bool\n",
    "        whether to do subsampling\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, only_word: bool=False, min_freq: int=5, do_subsampling: bool=True, t: float=1e-5):\n",
    "        self.only_word = only_word\n",
    "        self.min_freq = min_freq\n",
    "        self.do_subsampling = do_subsampling\n",
    "        self.t = t\n",
    "    \n",
    "    def process(self, corpus: List[str]):\n",
    "        \n",
    "        word_dic = set()\n",
    "        counter = Counter()\n",
    "        processed_sentence = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "        \n",
    "            counter.update(sentence)\n",
    "            processed_sentence.append(sentence)\n",
    "    \n",
    "        word_cnt = dict(filter(lambda x: x[1] > self.min_freq, counter.items()))\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(word_cnt.keys())}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.word_frequency = word_cnt.copy()\n",
    "        \n",
    "        #將文本轉為ID型式與移除文本中頻率過小的文字\n",
    "        self.processed_corpus = [[self.word2idx[word] for word in line if word in self.word2idx] for line in processed_sentence]\n",
    "        self.total_num_words = sum(len(line) for line in self.processed_corpus)\n",
    "        print(f\"Before subsampling: {self.total_num_words} words\")\n",
    "        \n",
    "        # 進行二次採樣(subsampling)\n",
    "        if self.do_subsampling:\n",
    "            self.processed_corpus = [[idx for idx in line if self.subsampling(idx)] for line in self.processed_corpus]\n",
    "            self.total_num_words = sum([len(line) for line in self.processed_corpus])\n",
    "            counter = Counter([self.idx2word[idx] for line in self.processed_corpus for idx in line])\n",
    "            word_cnt = dict(counter.items())\n",
    "            self.word_frequency = word_cnt.copy()\n",
    "            print(f\"After subsampling: {self.total_num_words} words\")\n",
    "        \n",
    "        self.processed_corpus = [[idx for idx in line] for line in self.processed_corpus if len(line) != 0]\n",
    "        \n",
    "        return self.processed_corpus, self.word2idx, self.idx2word, self.word_frequency, self.total_num_words\n",
    "    \n",
    "    def subsampling(self, idx):\n",
    "        \n",
    "        p = self.t / (self.word_frequency[self.idx2word[idx]] / self.total_num_words)\n",
    "        p_w = np.sqrt(p) + p\n",
    "        return random.uniform(0, 1) < p_w\n",
    "\n",
    "\n",
    "# 進行資料前處理\n",
    "# 這邊我們subsampling的t取1e-4\n",
    "pre_processor = PreProcessor(True, 5, True, 1e-4)\n",
    "corpus, word2idx, idx2word, word2freq, total_num_words = pre_processor.process(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfDuJuT5Kkvl"
   },
   "source": [
    "### 定義Skip-gram使用的Dataset與collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1905,
     "status": "ok",
     "timestamp": 1606320765981,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "DraniEYMKfWl"
   },
   "outputs": [],
   "source": [
    "# 客製化Dataset\n",
    "class SkipGramGetAllDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, corpus, word2freq, word2idx, idx2word, window_size, num_negatives):\n",
    "        self.corpus = corpus\n",
    "        self.word2freq = word2freq\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.window_size = window_size\n",
    "        self.num_negatives = num_negatives\n",
    "        \n",
    "        self.all_targets, self.all_contexts = self._get_all_contexts_targets()\n",
    "        self.all_negatives = self._get_all_negatives()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # 返回 目標字詞，上下文，負採樣樣本\n",
    "        return (self.all_targets[idx], self.all_contexts[idx], self.all_negatives[idx])\n",
    "        \n",
    "    \n",
    "    def _get_all_contexts_targets(self):\n",
    "        all_targets = []\n",
    "        all_contexts = []\n",
    "        \n",
    "        for line in self.corpus:\n",
    "            if len(line) < 2*self.window_size + 1:\n",
    "                continue\n",
    "            \n",
    "            all_contexts += line[self.window_size:-self.window_size]\n",
    "            for index in range(self.window_size, len(line) - self.window_size):\n",
    "                indices = list(range(max(0, index - self.window_size), min(len(line), index + self.window_size + 1)))\n",
    "                indices.remove(index)\n",
    "                all_targets.append([line[idx] for idx in indices])\n",
    "                               \n",
    "        return all_targets, all_contexts\n",
    "                               \n",
    "    \n",
    "    def _get_all_negatives(self):\n",
    "        \n",
    "        # hint: 進行負採樣，若沒頭緒的學員可以參考實作範例\n",
    "        \n",
    "        cur_exists_words = list(self.word2freq.keys())\n",
    "        sampling_weights = [self.word2freq[word]**0.75 for word in cur_exists_words]\n",
    "        population = list(range(len(sampling_weights)))\n",
    "        \n",
    "        all_negatives = []\n",
    "        neg_candidate = []\n",
    "        i = 0\n",
    "        for targets in self.all_targets:\n",
    "            negatives = []\n",
    "            while len(negatives) < self.num_negatives:  \n",
    "                if i == len(neg_candidate):\n",
    "                    neg_candidate = random.choices(population, sampling_weights, k=100000)\n",
    "                    neg_candidate = list(map(lambda x: self.word2idx[cur_exists_words[x]], neg_candidate))\n",
    "                    i = 0\n",
    "                if neg_candidate[i] != targets:\n",
    "                    negatives.append(neg_candidate[i])\n",
    "                i += 1\n",
    "            all_negatives.append(negatives)\n",
    "                \n",
    "        return all_negatives\n",
    "    \n",
    "# 客製化collate_fn\n",
    "def skipgram_collate(data):\n",
    "    contexts = []\n",
    "    target_negative = []\n",
    "    labels = []\n",
    "    for target, context, negative in data:\n",
    "        contexts += [context]\n",
    "        target_negative += [target + negative]\n",
    "        labels += [[1]*len(target) + [0]*len(negative)]\n",
    "        \n",
    "    return torch.tensor(contexts), torch.tensor(target_negative), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s94kJ0lKKzG5"
   },
   "source": [
    "### 定義Skip-gram模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1606320766292,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "kyyQyLxcKpv1"
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.in_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.out_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "    def forward(self, contexts, targets):\n",
    "        v = self.in_embedding(contexts).squeeze()\n",
    "        u = self.out_embedding(targets)\n",
    "\n",
    "        # do dot product to get output\n",
    "        pred = torch.matmul(v[:,None,:], u.permute(0,2,1))\n",
    "        \n",
    "        return pred.squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHZIFz7yK5An"
   },
   "source": [
    "### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 13745,
     "status": "ok",
     "timestamp": 1606320780465,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "Hr4sVBd8K10T"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "verbose = True\n",
    "num_epochs = 100\n",
    "batch_size = 512\n",
    "embed_size = 100\n",
    "lr = 0.01\n",
    "\n",
    "model = SkipGram(len(word2idx), embed_size)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "dataset = SkipGramGetAllDataset(corpus, word2freq, word2idx, idx2word, 2, 5)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=skipgram_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219482,
     "status": "ok",
     "timestamp": 1606321001876,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "sE28LW2_LB0I",
    "outputId": "4cf7c434-b5b8-4e73-eb3c-c2735ef0d17c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a9cebb284a4216a900895dc0812cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Batch: 501/537.080078125 Loss: 2.95766\n",
      "Epoch: 1/100, Loss: 2.89717\n",
      "Epoch: 2/100, Batch: 501/537.080078125 Loss: 1.19588\n",
      "Epoch: 2/100, Loss: 1.18683\n",
      "Epoch: 3/100, Batch: 501/537.080078125 Loss: 0.72332\n",
      "Epoch: 3/100, Loss: 0.72442\n",
      "Epoch: 4/100, Batch: 501/537.080078125 Loss: 0.56679\n",
      "Epoch: 4/100, Loss: 0.56924\n",
      "Epoch: 5/100, Batch: 501/537.080078125 Loss: 0.49788\n",
      "Epoch: 5/100, Loss: 0.50050\n",
      "Epoch: 6/100, Batch: 501/537.080078125 Loss: 0.45798\n",
      "Epoch: 6/100, Loss: 0.46047\n",
      "Epoch: 7/100, Batch: 501/537.080078125 Loss: 0.42913\n",
      "Epoch: 7/100, Loss: 0.43183\n",
      "Epoch: 8/100, Batch: 501/537.080078125 Loss: 0.40634\n",
      "Epoch: 8/100, Loss: 0.40937\n",
      "Epoch: 9/100, Batch: 501/537.080078125 Loss: 0.38758\n",
      "Epoch: 9/100, Loss: 0.39048\n",
      "Epoch: 10/100, Batch: 501/537.080078125 Loss: 0.37146\n",
      "Epoch: 10/100, Loss: 0.37434\n",
      "Epoch: 11/100, Batch: 501/537.080078125 Loss: 0.35772\n",
      "Epoch: 11/100, Loss: 0.36067\n",
      "Epoch: 12/100, Batch: 501/537.080078125 Loss: 0.34582\n",
      "Epoch: 12/100, Loss: 0.34867\n",
      "Epoch: 13/100, Batch: 501/537.080078125 Loss: 0.33540\n",
      "Epoch: 13/100, Loss: 0.33822\n",
      "Epoch: 14/100, Batch: 501/537.080078125 Loss: 0.32648\n",
      "Epoch: 14/100, Loss: 0.32916\n",
      "Epoch: 15/100, Batch: 501/537.080078125 Loss: 0.31851\n",
      "Epoch: 15/100, Loss: 0.32106\n",
      "Epoch: 16/100, Batch: 501/537.080078125 Loss: 0.31154\n",
      "Epoch: 16/100, Loss: 0.31423\n",
      "Epoch: 17/100, Batch: 501/537.080078125 Loss: 0.30583\n",
      "Epoch: 17/100, Loss: 0.30841\n",
      "Epoch: 18/100, Batch: 501/537.080078125 Loss: 0.30078\n",
      "Epoch: 18/100, Loss: 0.30336\n",
      "Epoch: 19/100, Batch: 501/537.080078125 Loss: 0.29596\n",
      "Epoch: 19/100, Loss: 0.29848\n",
      "Epoch: 20/100, Batch: 501/537.080078125 Loss: 0.29226\n",
      "Epoch: 20/100, Loss: 0.29454\n",
      "Epoch: 21/100, Batch: 501/537.080078125 Loss: 0.28929\n",
      "Epoch: 21/100, Loss: 0.29150\n",
      "Epoch: 22/100, Batch: 501/537.080078125 Loss: 0.28579\n",
      "Epoch: 22/100, Loss: 0.28804\n",
      "Epoch: 23/100, Batch: 501/537.080078125 Loss: 0.28253\n",
      "Epoch: 23/100, Loss: 0.28508\n",
      "Epoch: 24/100, Batch: 501/537.080078125 Loss: 0.28044\n",
      "Epoch: 24/100, Loss: 0.28261\n",
      "Epoch: 25/100, Batch: 501/537.080078125 Loss: 0.27822\n",
      "Epoch: 25/100, Loss: 0.28033\n",
      "Epoch: 26/100, Batch: 501/537.080078125 Loss: 0.27557\n",
      "Epoch: 26/100, Loss: 0.27804\n",
      "Epoch: 27/100, Batch: 501/537.080078125 Loss: 0.27383\n",
      "Epoch: 27/100, Loss: 0.27613\n",
      "Epoch: 28/100, Batch: 501/537.080078125 Loss: 0.27227\n",
      "Epoch: 28/100, Loss: 0.27433\n",
      "Epoch: 29/100, Batch: 501/537.080078125 Loss: 0.26982\n",
      "Epoch: 29/100, Loss: 0.27263\n",
      "Epoch: 30/100, Batch: 501/537.080078125 Loss: 0.26869\n",
      "Epoch: 30/100, Loss: 0.27103\n",
      "Epoch: 31/100, Batch: 501/537.080078125 Loss: 0.26772\n",
      "Epoch: 31/100, Loss: 0.26972\n",
      "Epoch: 32/100, Batch: 501/537.080078125 Loss: 0.26630\n",
      "Epoch: 32/100, Loss: 0.26848\n",
      "Epoch: 33/100, Batch: 501/537.080078125 Loss: 0.26522\n",
      "Epoch: 33/100, Loss: 0.26749\n",
      "Epoch: 34/100, Batch: 501/537.080078125 Loss: 0.26407\n",
      "Epoch: 34/100, Loss: 0.26630\n",
      "Epoch: 35/100, Batch: 501/537.080078125 Loss: 0.26265\n",
      "Epoch: 35/100, Loss: 0.26501\n",
      "Epoch: 36/100, Batch: 501/537.080078125 Loss: 0.26219\n",
      "Epoch: 36/100, Loss: 0.26442\n",
      "Epoch: 37/100, Batch: 501/537.080078125 Loss: 0.26125\n",
      "Epoch: 37/100, Loss: 0.26338\n",
      "Epoch: 38/100, Batch: 501/537.080078125 Loss: 0.25972\n",
      "Epoch: 38/100, Loss: 0.26181\n",
      "Epoch: 39/100, Batch: 501/537.080078125 Loss: 0.25918\n",
      "Epoch: 39/100, Loss: 0.26150\n",
      "Epoch: 40/100, Batch: 501/537.080078125 Loss: 0.25888\n",
      "Epoch: 40/100, Loss: 0.26073\n",
      "Epoch: 41/100, Batch: 501/537.080078125 Loss: 0.25777\n",
      "Epoch: 41/100, Loss: 0.26007\n",
      "Epoch: 42/100, Batch: 501/537.080078125 Loss: 0.25715\n",
      "Epoch: 42/100, Loss: 0.25932\n",
      "Epoch: 43/100, Batch: 501/537.080078125 Loss: 0.25660\n",
      "Epoch: 43/100, Loss: 0.25894\n",
      "Epoch: 44/100, Batch: 501/537.080078125 Loss: 0.25599\n",
      "Epoch: 44/100, Loss: 0.25795\n",
      "Epoch: 45/100, Batch: 501/537.080078125 Loss: 0.25495\n",
      "Epoch: 45/100, Loss: 0.25697\n",
      "Epoch: 46/100, Batch: 501/537.080078125 Loss: 0.25498\n",
      "Epoch: 46/100, Loss: 0.25699\n",
      "Epoch: 47/100, Batch: 501/537.080078125 Loss: 0.25421\n",
      "Epoch: 47/100, Loss: 0.25631\n",
      "Epoch: 48/100, Batch: 501/537.080078125 Loss: 0.25377\n",
      "Epoch: 48/100, Loss: 0.25590\n",
      "Epoch: 49/100, Batch: 501/537.080078125 Loss: 0.25306\n",
      "Epoch: 49/100, Loss: 0.25541\n",
      "Epoch: 50/100, Batch: 501/537.080078125 Loss: 0.25334\n",
      "Epoch: 50/100, Loss: 0.25536\n",
      "Epoch: 51/100, Batch: 501/537.080078125 Loss: 0.25223\n",
      "Epoch: 51/100, Loss: 0.25445\n",
      "Epoch: 52/100, Batch: 501/537.080078125 Loss: 0.25230\n",
      "Epoch: 52/100, Loss: 0.25445\n",
      "Epoch: 53/100, Batch: 501/537.080078125 Loss: 0.25102\n",
      "Epoch: 53/100, Loss: 0.25333\n",
      "Epoch: 54/100, Batch: 501/537.080078125 Loss: 0.25068\n",
      "Epoch: 54/100, Loss: 0.25287\n",
      "Epoch: 55/100, Batch: 501/537.080078125 Loss: 0.25102\n",
      "Epoch: 55/100, Loss: 0.25285\n",
      "Epoch: 56/100, Batch: 501/537.080078125 Loss: 0.25044\n",
      "Epoch: 56/100, Loss: 0.25264\n",
      "Epoch: 57/100, Batch: 501/537.080078125 Loss: 0.25024\n",
      "Epoch: 57/100, Loss: 0.25217\n",
      "Epoch: 58/100, Batch: 501/537.080078125 Loss: 0.24966\n",
      "Epoch: 58/100, Loss: 0.25181\n",
      "Epoch: 59/100, Batch: 501/537.080078125 Loss: 0.24929\n",
      "Epoch: 59/100, Loss: 0.25146\n",
      "Epoch: 60/100, Batch: 501/537.080078125 Loss: 0.24904\n",
      "Epoch: 60/100, Loss: 0.25135\n",
      "Epoch: 61/100, Batch: 501/537.080078125 Loss: 0.24925\n",
      "Epoch: 61/100, Loss: 0.25154\n",
      "Epoch: 62/100, Batch: 501/537.080078125 Loss: 0.24869\n",
      "Epoch: 62/100, Loss: 0.25086\n",
      "Epoch: 63/100, Batch: 501/537.080078125 Loss: 0.24837\n",
      "Epoch: 63/100, Loss: 0.25035\n",
      "Epoch: 64/100, Batch: 501/537.080078125 Loss: 0.24750\n",
      "Epoch: 64/100, Loss: 0.24970\n",
      "Epoch: 65/100, Batch: 501/537.080078125 Loss: 0.24778\n",
      "Epoch: 65/100, Loss: 0.24961\n",
      "Epoch: 66/100, Batch: 501/537.080078125 Loss: 0.24697\n",
      "Epoch: 66/100, Loss: 0.24926\n",
      "Epoch: 67/100, Batch: 501/537.080078125 Loss: 0.24733\n",
      "Epoch: 67/100, Loss: 0.24947\n",
      "Epoch: 68/100, Batch: 501/537.080078125 Loss: 0.24685\n",
      "Epoch: 68/100, Loss: 0.24904\n",
      "Epoch: 69/100, Batch: 501/537.080078125 Loss: 0.24694\n",
      "Epoch: 69/100, Loss: 0.24930\n",
      "Epoch: 70/100, Batch: 501/537.080078125 Loss: 0.24646\n",
      "Epoch: 70/100, Loss: 0.24869\n",
      "Epoch: 71/100, Batch: 501/537.080078125 Loss: 0.24705\n",
      "Epoch: 71/100, Loss: 0.24895\n",
      "Epoch: 72/100, Batch: 501/537.080078125 Loss: 0.24587\n",
      "Epoch: 72/100, Loss: 0.24799\n",
      "Epoch: 73/100, Batch: 501/537.080078125 Loss: 0.24586\n",
      "Epoch: 73/100, Loss: 0.24814\n",
      "Epoch: 74/100, Batch: 501/537.080078125 Loss: 0.24581\n",
      "Epoch: 74/100, Loss: 0.24815\n",
      "Epoch: 75/100, Batch: 501/537.080078125 Loss: 0.24523\n",
      "Epoch: 75/100, Loss: 0.24771\n",
      "Epoch: 76/100, Batch: 501/537.080078125 Loss: 0.24574\n",
      "Epoch: 76/100, Loss: 0.24774\n",
      "Epoch: 77/100, Batch: 501/537.080078125 Loss: 0.24515\n",
      "Epoch: 77/100, Loss: 0.24746\n",
      "Epoch: 78/100, Batch: 501/537.080078125 Loss: 0.24497\n",
      "Epoch: 78/100, Loss: 0.24717\n",
      "Epoch: 79/100, Batch: 501/537.080078125 Loss: 0.24523\n",
      "Epoch: 79/100, Loss: 0.24711\n",
      "Epoch: 80/100, Batch: 501/537.080078125 Loss: 0.24492\n",
      "Epoch: 80/100, Loss: 0.24698\n",
      "Epoch: 81/100, Batch: 501/537.080078125 Loss: 0.24448\n",
      "Epoch: 81/100, Loss: 0.24647\n",
      "Epoch: 82/100, Batch: 501/537.080078125 Loss: 0.24393\n",
      "Epoch: 82/100, Loss: 0.24629\n",
      "Epoch: 83/100, Batch: 501/537.080078125 Loss: 0.24391\n",
      "Epoch: 83/100, Loss: 0.24603\n",
      "Epoch: 84/100, Batch: 501/537.080078125 Loss: 0.24425\n",
      "Epoch: 84/100, Loss: 0.24619\n",
      "Epoch: 85/100, Batch: 501/537.080078125 Loss: 0.24387\n",
      "Epoch: 85/100, Loss: 0.24600\n",
      "Epoch: 86/100, Batch: 501/537.080078125 Loss: 0.24361\n",
      "Epoch: 86/100, Loss: 0.24596\n",
      "Epoch: 87/100, Batch: 501/537.080078125 Loss: 0.24402\n",
      "Epoch: 87/100, Loss: 0.24600\n",
      "Epoch: 88/100, Batch: 501/537.080078125 Loss: 0.24332\n",
      "Epoch: 88/100, Loss: 0.24516\n",
      "Epoch: 89/100, Batch: 501/537.080078125 Loss: 0.24321\n",
      "Epoch: 89/100, Loss: 0.24543\n",
      "Epoch: 90/100, Batch: 501/537.080078125 Loss: 0.24301\n",
      "Epoch: 90/100, Loss: 0.24506\n",
      "Epoch: 91/100, Batch: 501/537.080078125 Loss: 0.24293\n",
      "Epoch: 91/100, Loss: 0.24499\n",
      "Epoch: 92/100, Batch: 501/537.080078125 Loss: 0.24287\n",
      "Epoch: 92/100, Loss: 0.24497\n",
      "Epoch: 93/100, Batch: 501/537.080078125 Loss: 0.24267\n",
      "Epoch: 93/100, Loss: 0.24515\n",
      "Epoch: 94/100, Batch: 501/537.080078125 Loss: 0.24266\n",
      "Epoch: 94/100, Loss: 0.24461\n",
      "Epoch: 95/100, Batch: 501/537.080078125 Loss: 0.24251\n",
      "Epoch: 95/100, Loss: 0.24471\n",
      "Epoch: 96/100, Batch: 501/537.080078125 Loss: 0.24233\n",
      "Epoch: 96/100, Loss: 0.24461\n",
      "Epoch: 97/100, Batch: 501/537.080078125 Loss: 0.24233\n",
      "Epoch: 97/100, Loss: 0.24440\n",
      "Epoch: 98/100, Batch: 501/537.080078125 Loss: 0.24256\n",
      "Epoch: 98/100, Loss: 0.24459\n",
      "Epoch: 99/100, Batch: 501/537.080078125 Loss: 0.24181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99/100, Loss: 0.24398\n",
      "Epoch: 100/100, Batch: 501/537.080078125 Loss: 0.24200\n",
      "Epoch: 100/100, Loss: 0.24404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "lst_loss = []\n",
    "model.train()\n",
    "for epc in tqdm(range(num_epochs)):\n",
    "    batch_loss = 0\n",
    "\n",
    "    for i, (contexts, target_negative, labels) in enumerate(loader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_cuda:\n",
    "            contexts = contexts.cuda()\n",
    "            target_negative = target_negative.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        pred = model(contexts, target_negative)\n",
    "        loss = criterion(pred.float(), labels.float())\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(f\"Epoch: {epc + 1}/{num_epochs}, Batch: {i+1}/{len(dataset)/batch_size} Loss: {batch_loss / i:.5f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Epoch: {epc + 1}/{num_epochs}, Loss: {batch_loss / i:.5f}\")\n",
    "    \n",
    "    lst_loss.append(batch_loss/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 728,
     "status": "ok",
     "timestamp": 1606321013487,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "y0rt5W2ELLvP",
    "outputId": "b497edcc-fc8e-47d3-b3ff-c574d42ff581"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization loss\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lst_loss, marker='s')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Word2Vec Skip-gram Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 730,
     "status": "ok",
     "timestamp": 1606321030038,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "43pOYRh-MX_F",
    "outputId": "3de0675b-47af-462d-c927-f14a62933956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.383: cosby.\n",
      "cosine sim=0.360: entertaining.\n",
      "cosine sim=0.334: occasionally.\n",
      "cosine sim=0.331: they.\n"
     ]
    }
   ],
   "source": [
    "#計算字詞相似度\n",
    "\n",
    "def get_similarity(word, top_k, model, word2idx, idx2word):\n",
    "    W = (model.in_embedding.weight.data + model.out_embedding.weight.data) / 2\n",
    "    idx = word2idx.get(word, None)\n",
    "    \n",
    "    if not idx:\n",
    "        # 當出現不在字典中的字詞時，顯示Out of vocabulary error\n",
    "        raise ValueError(\"Out of vocabulary\")\n",
    "    else:\n",
    "        x = W[idx]\n",
    "        \n",
    "        # 使用cosine相似計算字詞間的相似程度\n",
    "        cos = torch.matmul(W, x) / (torch.sum(W * W, dim=-1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "        _, topk = torch.topk(cos, top_k+1)\n",
    "        \n",
    "        for i in topk[1:]:\n",
    "            print(f\"cosine sim={cos[int(i)]:.3f}: {idx2word[int(i)]}.\")\n",
    "\n",
    "get_similarity('love', 4, model, word2idx, idx2word)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNRbZbSHSpiMTWmQCCagSqg",
   "collapsed_sections": [],
   "name": "word2vec高速化_作業解答.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
