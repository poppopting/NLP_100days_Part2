{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day33_BERT_Tutorial_1_中間字預測(作業2範例).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c1796eb69cc94e4ba2ceed869ab2a55c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f6db1c6433f3471c872aae25c890e824","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1d9356d1321541bbb1dc5cdb28939118","IPY_MODEL_6321f36dd72145cbaf442762d7ffac85"]}},"f6db1c6433f3471c872aae25c890e824":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1d9356d1321541bbb1dc5cdb28939118":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8276ea77f43842b894e0c3aa4cbc600f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c2d6ff3b93094db5a0a9d058fc9a91d5"}},"6321f36dd72145cbaf442762d7ffac85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_51680a93657440e8b665290a5467a81d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 729kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8eff42de0dac474c913420a4d0ec67e0"}},"8276ea77f43842b894e0c3aa4cbc600f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c2d6ff3b93094db5a0a9d058fc9a91d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"51680a93657440e8b665290a5467a81d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8eff42de0dac474c913420a4d0ec67e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73b887ae874a4d47ae968b4457e52844":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f2c2df4aac8942b88f19c2635d1ca75e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_641fbe38012b4c8fa71e1e335eef2cad","IPY_MODEL_29cab1e3e7f54f3c90731fd72dc2e752"]}},"f2c2df4aac8942b88f19c2635d1ca75e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"641fbe38012b4c8fa71e1e335eef2cad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ff0d0de5ce7e4c31916f8f4b6be315a3","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0d4879bb92d44f50a5d67f3b0e4eb3a4"}},"29cab1e3e7f54f3c90731fd72dc2e752":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_99f8bc45b8e04ce596fa5944e9b84e07","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 302B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ca529ce038b24d78a8253057663bb3d0"}},"ff0d0de5ce7e4c31916f8f4b6be315a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0d4879bb92d44f50a5d67f3b0e4eb3a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"99f8bc45b8e04ce596fa5944e9b84e07":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ca529ce038b24d78a8253057663bb3d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ef6b1ad642b5400fbd9fff20a46176e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fe1d05d8386b4ffaa3945a16c7922ce1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_54085d68bece4334ad27516f1fa28c3d","IPY_MODEL_d8a18270e96c4469baabd0a5f5efc891"]}},"fe1d05d8386b4ffaa3945a16c7922ce1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"54085d68bece4334ad27516f1fa28c3d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b9477fc23490429580ba8198d62678f0","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e2159ba72605457c9701afdc959dd269"}},"d8a18270e96c4469baabd0a5f5efc891":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_90437ea1ff6c4f5dae45753b2c8080cc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 4.82MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5a7417eb72e145559c71a076dcbc6fb6"}},"b9477fc23490429580ba8198d62678f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e2159ba72605457c9701afdc959dd269":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"90437ea1ff6c4f5dae45753b2c8080cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5a7417eb72e145559c71a076dcbc6fb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d36347ed172e4034bd5686994b1b3a16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_63f365f795874f728c2c7c283deb564e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d944d9880b8746e2a93e981f5ae5f415","IPY_MODEL_19f6a118743541a3bd55a3c3600728bb"]}},"63f365f795874f728c2c7c283deb564e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d944d9880b8746e2a93e981f5ae5f415":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_03587e772d1449f088f6fb18c7576c83","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_76fb54b29efa4fd190225ae87f5dd638"}},"19f6a118743541a3bd55a3c3600728bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_360ddddb20d1406fb01df999e2713dfb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:00&lt;00:00, 5.72kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_429651ff515c429496c5b005642332b0"}},"03587e772d1449f088f6fb18c7576c83":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"76fb54b29efa4fd190225ae87f5dd638":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"360ddddb20d1406fb01df999e2713dfb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"429651ff515c429496c5b005642332b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"98268a20431d4e919d51cd60e437f2fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_753db1178b734e7b875a4240999a7c15","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_273c5b99e80f4a81b7e408eb8a728dce","IPY_MODEL_e20bf72b2e8f4918a1e62f55d947f6b6"]}},"753db1178b734e7b875a4240999a7c15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"273c5b99e80f4a81b7e408eb8a728dce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a828f562566b47c8994b4fbf80f9c179","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_287e52c85fe446578f75317a506176b2"}},"e20bf72b2e8f4918a1e62f55d947f6b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bad56a2ae55b4494843948aea1801d28","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:09&lt;00:00, 44.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d572eff337f6444ba2c3efd94c16098f"}},"a828f562566b47c8994b4fbf80f9c179":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"287e52c85fe446578f75317a506176b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bad56a2ae55b4494843948aea1801d28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d572eff337f6444ba2c3efd94c16098f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"IsIU8GXSUQDI"},"source":["# **Google BERT Transformers with Pytorch Example** "]},{"cell_type":"markdown","metadata":{"id":"NAUXCyYdUtnE"},"source":["**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**\n","\n","\n","PyTorch Transformers can be installed using pip below and few pre-requisites:\n","*   So change the Run Time type to GPU \n","*   Use version Python 3.5+ and PyTorch 1.1.0\n","*   Import the required Libraries\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VA_5qKN0iSVl"},"source":["## What is BERT?\n","A new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be fine- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- specific architecture modifications."]},{"cell_type":"markdown","metadata":{"id":"87HjTjkqAq_K"},"source":["There are a few things I want to explain in this section.\n","\n","1. It’s easy to get that BERT stands for Bidirectional Encoder Representations from Transformers. Each word here has a meaning to it and we will encounter that one by one. For now, the key takeaway from this line is – BERT is based on the Transformer architecture.\n","\n","2. BERT is pre-trained on a large corpus of unlabelled text including the entire Wikipedia(that’s 2,500 million words!) and Book Corpus (800 million words). This pretraining step is really important for BERT's success. This is because as we train a model on a large text corpus, our model starts to pick up the deeper and intimate understandings of how the language works. This knowledge is the swiss army knife that is useful for almost any NLP task.\n","\n","3. BERT is a deeply bidirectional model. Bidirectional means that BERT learns information from both the left and the right side of a token’s context during the training phase."]},{"cell_type":"markdown","metadata":{"id":"Y2cAxP2uAX5P"},"source":["BERT was built upon recent work in pre-training contextual representations — including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit — but crucially these models are all unidirectional or shallowly bidirectional. This means that each word is only contextualized using the words to its left (or right). \n","\n","\n","\n","For example, in the sentence I made a bank deposit the unidirectional representation of bank is only based on I made a but not deposit. Some previous work does combine the representations from separate left-context and right-context models, but only in a \"shallow\" manner.\n","\n"," **Example**- BERT represents \"bank\" using both its left and right context — \n"," **I made a ... deposit** — starting from the very bottom of a deep neural network, so it is deeply bidirectional."]},{"cell_type":"markdown","metadata":{"id":"-3IZV5jjBttx"},"source":["![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/09/sent_context.png)"]},{"cell_type":"markdown","metadata":{"id":"Mgo0da8mCGB2"},"source":["## Main concepts\n","\n","The library is build around three type of classes for each models:\n","\n","1. **model classes** which are PyTorch models (torch.nn.Modules) of the 8 models architectures currently provided in the library, e.g. BertModel\n","2. **configuration classes** which store all the parameters required to build a model, e.g. BertConfig. You don’t always need to instantiate these your-self, in particular if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model)\n","3. **tokenizer classes** which store the vocabulary for each model and provide methods for encoding/decoding strings in list of token embeddings indices to be fed to a model, e.g. BertTokenizer\n","All these classes can be instantiated from pretrained instances and saved locally using two methods:\n","\n","**from_pretrained()** let you instantiate a model/configuration/tokenizer from a pretrained version either provided by the library itself (currently 27 models are provided as listed here) or stored locally (or on a server) by the user.\n","\n","**save_pretrained()** let you save a model/configuration/tokenizer locally so that it can be reloaded using from_pretrained()."]},{"cell_type":"markdown","metadata":{"id":"3Go6_nxwchPB"},"source":["### 1. Bert Model Architecture\n","\n","BERT’s model architecture is a multi-layer bidirectional Transformer encoder\n","\n","1. **BERT-Large, Uncased (Whole Word Masking)**: 24-layer, 1024-hidden, 16-heads, 340M parameters\n","2. **BERT-Large, Cased (Whole Word Masking)**: 24-layer, 1024-hidden, 16-heads, 340M parameters\n","3. **BERT-Base, Uncased**: 12-layer, 768-hidden, 12-heads, 110M parameters\n","4. **BERT-Large, Uncased**: 24-layer, 1024-hidden, 16-heads, 340M parameters\n","5. **BERT-Base, Cased**: 12-layer, 768-hidden, 12-heads , 110M parameters\n","6. **BERT-Large, Cased**: 24-layer, 1024-hidden, 16-heads, 340M parameters"]},{"cell_type":"markdown","metadata":{"id":"Gs1mCcR-gWC0"},"source":["![](http://jalammar.github.io/images/bert-base-bert-large-encoders.png)"]},{"cell_type":"markdown","metadata":{"id":"HVA8QwkmkJAI"},"source":["We denote the number of layers (i.e., Transformer blocks) \n","as **L**, the **hidden size** as **H**, and \n","the number of self-attention heads as We primarily report results on \n","two model sizes: \n","1. **BERTBASE (L=12, H=768, A=12, Total Parameters=110M)** \n","2. **BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)**"]},{"cell_type":"markdown","metadata":{"id":"CXhYHZbhVh92"},"source":["Load pre-trained model tokenizer (vocabulary)\n","The base class PreTrainedTokenizer implements the common methods for loading/saving a tokenizer either from a local file or directory, or from a pretrained tokenizer provided by the library (downloaded from HuggingFace’s AWS S3 repository)."]},{"cell_type":"markdown","metadata":{"id":"7pPb2PwZFwZB"},"source":["```\n","class transformers.BertModel\n","```\n","Parameters\n","*   config (BertConfig) – Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model weights.\n","\n","**Example**\n","Indices of input sequence tokens in the vocabulary. To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n","\n","1. **For sequence pairs:**\n","\n","tokens: [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","\n","token_type_ids: 0   0  0    0    0    0    0   0   1  1  1  1 1   1\n","\n","2. **For single sequences:**\n","\n","tokens:[CLS] the dog is hairy . [SEP]\n","\n","token_type_ids:   0   0   0   0  0     0   0\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nqoLtjmRVzzV"},"source":["### 2. Bert Tokenizer\n","**BertTokenizer** = Tokenizer classes which store the vocabulary for each model and provide methods for encoding/decoding strings in list of token embeddings indices to be fed to a model eg DistilBertTokenizer,BertTokenizer etc\n","![](https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-1.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0VHjPc7rXoVy"},"source":["**PreTrainedTokenizer** is the main entry point into tokenizers as it also implements the main methods for using all the tokenizers:\n","\n","*   tokenizing, converting tokens to ids and back and encoding/decoding,\n","*   adding new tokens to the vocabulary in a way that is independant of the underlying structure (BPE, SentencePiece…),\n","*   managing special tokens (adding them, assigning them to roles, making sure they are not split during tokenization)\n","\n","**Main Class from where the Tokenizer is been called**\n","```\n","class transformers.BertTokenizer(vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, **kwargs)\n","\n","```\n","\n","*Parameters*\n","\n","\n","*  **vocab_file** – Path to a one-wordpiece-per-line vocabulary file\n","*  **do_lower_case** – Whether to lower case the input. Only has an effect when                     do_basic_tokenize=True\n","\n","*  **do_basic_tokenize** – Whether to do basic tokenization before wordpiece.\n","*  **max_len** – An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the minimum of this value (if             specified) and the underlying BERT model’s sequence length.\n","\n","*   **never_split** – List of tokens which will never be split during                        tokenization. Only has an effect when do_basic_tokenize=True\n"]},{"cell_type":"markdown","metadata":{"id":"_-Ji4oGwanPv"},"source":["![](https://www.researchgate.net/publication/323904682/figure/fig1/AS:606458626465792@1521602412057/The-Transformer-model-architecture.png)"]},{"cell_type":"markdown","metadata":{"id":"W8bBNBora1BJ"},"source":["So to have a detail architecture of how Encoder-Decoder works here is few [Link1](https://arxiv.org/pdf/1706.03762.pdf) & visual [Link2](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) "]},{"cell_type":"markdown","metadata":{"id":"FiXGe7Qlcy3v"},"source":["**ARCHITECTURE**: \n","1. **Encoder**: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n","\n","2. **Decoder:** The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i."]},{"cell_type":"markdown","metadata":{"id":"8RJLtIdAX5lX"},"source":["Let’s try to classify the sentence “a visually stunning rumination on love”. The first step is to use the BERT tokenizer to first split the word into tokens. Then, we add the special tokens needed for sentence classifications (these are [CLS] at the first position, and [SEP] at the end of the sentence).\n","\n","[CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques- tions/answers)."]},{"cell_type":"markdown","metadata":{"id":"3Fc-rFzfEDim"},"source":["![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/09/bert_emnedding.png)"]},{"cell_type":"markdown","metadata":{"id":"0t0i3LnbFNjN"},"source":["BERT input representation. The **input embeddings** are the sum of the **token embeddings**, the **segmentation embeddings** and **the position embeddings**."]},{"cell_type":"markdown","metadata":{"id":"85eRapWMEigo"},"source":["1. The first token of every sequence is always a special clas- sification token (**[CLS]**). \n","2. The final hidden state corresponding to this token is used as the ag- gregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence.\n","3. We differentiate the sentences in two ways. First, we separate them with a special token (**[SEP]**). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. \n","4.A positional embedding is also added to each token to indicate its position in the sequence.\n"]},{"cell_type":"markdown","metadata":{"id":"mePP9GjSCpm9"},"source":["### 3. Configuration Class\n"]},{"cell_type":"markdown","metadata":{"id":"Gf1CkpfrGy0g"},"source":["The base class PretrainedConfig implements the common methods for loading/saving a configuration either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace’s AWS S3 repository).\n","\n","\n","\n","```\n","class transformers.PretrainedConfig(**kwargs)\n","```\n","Base class for all configuration classes. Handles a few parameters common to all models’ configurations as well as methods for loading/downloading/saving configurations.\n","\n","*Parameters*\n","\n","*  **finetuning_task** – string, default None. Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow or PyTorch) checkpoint.\n","\n","*   **num_labels** – integer, default 2. Number of classes to use when the model is a classification model (sequences/tokens)\n","*   **output_hidden_states** – string, default False. Should the model returns all hidden-states.\n","\n","*   **output_attentions** – boolean, default False. Should the model returns attentions weights.\n","*   **torchscript** – string, default False. Is the model used with Torchscript.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WddVtaL9J92w"},"source":["# Model Building"]},{"cell_type":"markdown","metadata":{"id":"lZYdnHUpIklM"},"source":["Using BERT has two stages: Pre-training and fine-tuning.\n","\n","**Pre-training** :It is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a one-time procedure for each language (current models are English-only, but multilingual models will be released in the near future). We are releasing a number of pre-trained models from the paper which were pre-trained at Google. Most NLP researchers will never need to pre-train their own model from scratch.\n","\n","**Fine-tuning** : It is inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model. SQuAD, for example, can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%, which is the single system state-of-the-art.\n","\n","\n","![](https://insidebigdata.com/wp-content/uploads/2019/10/Peltarion_pic2.jpg)"]},{"cell_type":"markdown","metadata":{"id":"iITsqEiLJJGd"},"source":["Here we will see two pre-trained Models\n","> *Model 1*: **Masked LM**\n","\n","> *Model 2*: **Next Sentence Prediction (NSP)**"]},{"cell_type":"markdown","metadata":{"id":"khJSahp4bReO"},"source":["## Model 1:Masked LM"]},{"cell_type":"markdown","metadata":{"id":"LiV-JiwbbXE-"},"source":["**STEPS**\n","\n","1. Deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. \n"," the model could trivially predict the target word in a multi-layered context.\n","\n","2. In order to train a deep bidirectional representa- tion, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. \n","\n","3. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. \n","\n","\n","Although this allows us to obtain a bidirectional pretrained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the **[MASK]** token does not appear during fine-tuning. To mitigate this, we do not always replace “masked” words with the actual **[MASK]** token. \n","\n","**1 Problem:** Language models only use left context or right context, but language understanding is bidirectional.\n","\n","● Why are LMs unidirectional?\n","\n","*Reason 1:* Directionality is needed to generate a\n","well-formed probability distribution.\n","\n","    ○ We don’t care about this.\n","● \n","*Reason 2:* Words can “see themselves” in a bidirectional encoder.\n","\n","**Solution:** Mask out k% of the input words, \n","and then predict the masked words\n","○ We always use k = 15%\n","\n","\n","                                store                 gallon\n","                                  |                    |\n","          the man went to the **[MASK]**  to buy a **[MASK]**  of milk\n","\n","● Too little masking: Too expensive to train\n","\n","● Too much masking: Not enough context\n","\n","\n","\n","**2 Problem:** Mask token never seen at fine-tuning\n","\n","**Solution:** 15% of the words to predict, but don’t\n","replace with **[MASK]** 100% of the time. Instead:\n","\n","● 80% of the time, replace with **[MASK]** \n","went to the store → went to the **[MASK]** \n","\n","● 10% of the time, replace random word\n","went to the store → went to the running\n","\n","● 10% of the time, keep same\n","went to the store → went to the store\n","\n","\n","If the i-th token is chosen, we replace the i-th token with Then, Ti will be used to predict the original token with cross entropy loss.\n","\n"]},{"cell_type":"code","metadata":{"id":"AInZF3z5IOru","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231074815,"user_tz":-480,"elapsed":12810,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"bc87ef53-e47d-4b60-e7db-d7bd9550f7f4"},"source":["!pip install transformers\n","# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n","import logging\n","logging.basicConfig(level=logging.INFO)\n","import torch\n","from transformers import BertTokenizer, BertModel, BertForMaskedLM"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n","\r\u001b[K     |▏                               | 10kB 24.8MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 30.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 24.8MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 28.5MB/s eta 0:00:01\r\u001b[K     |▊                               | 51kB 26.6MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 29.1MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 18.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 81kB 19.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 92kB 18.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 102kB 18.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112kB 18.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 122kB 18.0MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 18.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 143kB 18.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 153kB 18.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 163kB 18.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 174kB 18.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 184kB 18.0MB/s eta 0:00:01\r\u001b[K     |███                             | 194kB 18.0MB/s eta 0:00:01\r\u001b[K     |███                             | 204kB 18.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 215kB 18.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 225kB 18.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 235kB 18.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 245kB 18.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 256kB 18.0MB/s eta 0:00:01\r\u001b[K     |████                            | 266kB 18.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 276kB 18.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 286kB 18.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 296kB 18.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 307kB 18.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 317kB 18.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 327kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 337kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 348kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 358kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 368kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 378kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 389kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 399kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 409kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 419kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 430kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 440kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 450kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 460kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 471kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 481kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 491kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 501kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 512kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 522kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 532kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 542kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 552kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 563kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 573kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 583kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 593kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 604kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 614kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 624kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 634kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 645kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 655kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 665kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 675kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 686kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 696kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 706kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 716kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 727kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 737kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 747kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 757kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 768kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 778kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 788kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 798kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 808kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 819kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 829kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 839kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 849kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 860kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 870kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 880kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 890kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 901kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 911kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 921kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 931kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 942kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 952kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 962kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 972kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 983kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 993kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.2MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.2MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.2MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.2MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.2MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.2MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.2MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.2MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.2MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.3MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.3MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.3MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.3MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.3MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.3MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.3MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.3MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.4MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.4MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.4MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.4MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.4MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.4MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.4MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.4MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.4MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.4MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.5MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.5MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.5MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.5MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.5MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.5MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.5MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.5MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.6MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.6MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.6MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.6MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.6MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.6MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.6MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.6MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.6MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.6MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.7MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.7MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.7MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.7MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.7MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.7MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.7MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.7MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.8MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.8MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.8MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.8MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.8MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.8MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.8MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.8MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.8MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.8MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.9MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.9MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.9MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.9MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.9MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.9MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.9MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.9MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.9MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.0MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.2MB 18.0MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 51.9MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n","\u001b[K     |████████████████████████████████| 870kB 49.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=c237e659bf16e546b77fb709f6dfbcadb8e4bd18f975f5875da62ac889410b15\n","  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Azh1mYBfcZKE","colab":{"base_uri":"https://localhost:8080/","height":284,"referenced_widgets":["c1796eb69cc94e4ba2ceed869ab2a55c","f6db1c6433f3471c872aae25c890e824","1d9356d1321541bbb1dc5cdb28939118","6321f36dd72145cbaf442762d7ffac85","8276ea77f43842b894e0c3aa4cbc600f","c2d6ff3b93094db5a0a9d058fc9a91d5","51680a93657440e8b665290a5467a81d","8eff42de0dac474c913420a4d0ec67e0","73b887ae874a4d47ae968b4457e52844","f2c2df4aac8942b88f19c2635d1ca75e","641fbe38012b4c8fa71e1e335eef2cad","29cab1e3e7f54f3c90731fd72dc2e752","ff0d0de5ce7e4c31916f8f4b6be315a3","0d4879bb92d44f50a5d67f3b0e4eb3a4","99f8bc45b8e04ce596fa5944e9b84e07","ca529ce038b24d78a8253057663bb3d0","ef6b1ad642b5400fbd9fff20a46176e8","fe1d05d8386b4ffaa3945a16c7922ce1","54085d68bece4334ad27516f1fa28c3d","d8a18270e96c4469baabd0a5f5efc891","b9477fc23490429580ba8198d62678f0","e2159ba72605457c9701afdc959dd269","90437ea1ff6c4f5dae45753b2c8080cc","5a7417eb72e145559c71a076dcbc6fb6"]},"executionInfo":{"status":"ok","timestamp":1618231081564,"user_tz":-480,"elapsed":1447,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"0c6fa701-fc86-490a-e919-b0e7c274dfdd"},"source":["# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["INFO:filelock:Lock 139810537052368 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1796eb69cc94e4ba2ceed869ab2a55c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:filelock:Lock 139810537052368 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n","INFO:filelock:Lock 139809153114704 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73b887ae874a4d47ae968b4457e52844","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:filelock:Lock 139809153114704 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n","INFO:filelock:Lock 139810598205840 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef6b1ad642b5400fbd9fff20a46176e8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:filelock:Lock 139810598205840 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sdEqm_R3ccHX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231084974,"user_tz":-480,"elapsed":1030,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"d9eb5de3-f06f-4395-b217-3dcc10fd683c"},"source":["# Tokenize input\n","text  = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n","tokenized_text = tokenizer.tokenize(text)\n","#tokenized_text.T\n","tokenized_text"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]',\n"," 'who',\n"," 'was',\n"," 'jim',\n"," 'henson',\n"," '?',\n"," '[SEP]',\n"," 'jim',\n"," 'henson',\n"," 'was',\n"," 'a',\n"," 'puppet',\n"," '##eer',\n"," '[SEP]']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"nOB1PfvSfZbM"},"source":["Here MASK is a word we will predict between the sentence\n","\n","\n","**TOKEN EMBEDDINGS**"]},{"cell_type":"code","metadata":{"id":"6IGr3YHRchsk","executionInfo":{"status":"ok","timestamp":1618231089088,"user_tz":-480,"elapsed":994,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}}},"source":["# Mask a token that we will try to predict back with `BertForMaskedLM`\n","masked_index = 8\n","tokenized_text[masked_index] = '[MASK]'\n","assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQME6Hyzc9bN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231091168,"user_tz":-480,"elapsed":825,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"2daffa60-89d7-4fde-a9c5-4fdc6eb40414"},"source":["tokenized_text"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]',\n"," 'who',\n"," 'was',\n"," 'jim',\n"," 'henson',\n"," '?',\n"," '[SEP]',\n"," 'jim',\n"," '[MASK]',\n"," 'was',\n"," 'a',\n"," 'puppet',\n"," '##eer',\n"," '[SEP]']"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"sYFFDHAXhDE3"},"source":["**POSITION EMBEDDINGS**"]},{"cell_type":"code","metadata":{"id":"prohNWKVcjfx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231100993,"user_tz":-480,"elapsed":1766,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"80ee1dac-d671-4c6d-8627-7d8126c11a1a"},"source":["# Convert token to vocabulary indices\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","indexed_tokens"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[101,\n"," 2040,\n"," 2001,\n"," 3958,\n"," 27227,\n"," 1029,\n"," 102,\n"," 3958,\n"," 103,\n"," 2001,\n"," 1037,\n"," 13997,\n"," 11510,\n"," 102]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"E5J4NYdQhln7"},"source":["**SEGMENT EMBEDDINGS**\n","\n","Here from the tokenized tokens which are part of one sentence we indexing with a 0,1 respectively for each sentence."]},{"cell_type":"code","metadata":{"id":"X8WapubcclMV","executionInfo":{"status":"ok","timestamp":1618231104049,"user_tz":-480,"elapsed":924,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}}},"source":["# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n","segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n","\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"djEHZYr7cm1G","executionInfo":{"status":"ok","timestamp":1618231105961,"user_tz":-480,"elapsed":628,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}}},"source":["# Convert inputs to PyTorch tensors\n","tokens_tensor = torch.tensor([indexed_tokens])\n","segments_tensors = torch.tensor([segments_ids])"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"09_n9_ifdZHS","colab":{"base_uri":"https://localhost:8080/","height":202,"referenced_widgets":["d36347ed172e4034bd5686994b1b3a16","63f365f795874f728c2c7c283deb564e","d944d9880b8746e2a93e981f5ae5f415","19f6a118743541a3bd55a3c3600728bb","03587e772d1449f088f6fb18c7576c83","76fb54b29efa4fd190225ae87f5dd638","360ddddb20d1406fb01df999e2713dfb","429651ff515c429496c5b005642332b0","98268a20431d4e919d51cd60e437f2fa","753db1178b734e7b875a4240999a7c15","273c5b99e80f4a81b7e408eb8a728dce","e20bf72b2e8f4918a1e62f55d947f6b6","a828f562566b47c8994b4fbf80f9c179","287e52c85fe446578f75317a506176b2","bad56a2ae55b4494843948aea1801d28","d572eff337f6444ba2c3efd94c16098f"]},"executionInfo":{"status":"ok","timestamp":1618231119727,"user_tz":-480,"elapsed":12958,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"099e9e39-b85b-4a5c-e48f-3f8b35186fa2"},"source":["# Load pre-trained model (weights)\n","model = BertModel.from_pretrained('bert-base-uncased')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["INFO:filelock:Lock 139809157835344 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170.lock\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d36347ed172e4034bd5686994b1b3a16","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:filelock:Lock 139809157835344 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170.lock\n","INFO:filelock:Lock 139809158674512 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98268a20431d4e919d51cd60e437f2fa","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:filelock:Lock 139809158674512 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fzLgAl3Ilti8"},"source":["**Hyperparameter Tuning**\n","  1. layer_norm_eps: 1e-12\n","  2. max_position_embeddings: 512\n","  3. num_attention_heads: 12\n","  4. num_hidden_layers: 12\n","  5. num_labels: 2\n","  10. torchscript: false\n","  11. type_vocab_size: 2\n","  13. vocab_size: 30522"]},{"cell_type":"code","metadata":{"id":"_vR9k4P5disP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231119730,"user_tz":-480,"elapsed":6979,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"c4efb7d2-e25c-47af-a8bd-7ffa77dec95d"},"source":["# Set the model in evaluation mode to deactivate the DropOut modules\n","# This is IMPORTANT to have reproducible results during evaluation!\n","model.eval()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"0BoPAjibdlkt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231131793,"user_tz":-480,"elapsed":17861,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"78d3169b-d407-42bf-a70d-440b713cfbda"},"source":["# If you have a GPU, put everything on cuda\n","tokens_tensor = tokens_tensor.to('cuda')\n","segments_tensors = segments_tensors.to('cuda')\n","model.to('cuda')\n"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"DcIqReM6d_nQ","executionInfo":{"status":"ok","timestamp":1618231131794,"user_tz":-480,"elapsed":17249,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}}},"source":["\n","# Predict hidden states features for each layer\n","with torch.no_grad():\n","    # See the models docstrings for the detail of the inputs\n","    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","    # Transformers models always output tuples.\n","    # See the models docstrings for the detail of all the outputs\n","    # In our case, the first element is the hidden state of the last layer of the Bert model\n","    encoded_layers = outputs[0]\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BLZ8_uifLaX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231131795,"user_tz":-480,"elapsed":16579,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"f6933746-57bc-426a-9a54-fbae766cd348"},"source":["encoded_layers.shape\n"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 14, 768])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"eW66dStfj-_V"},"source":[" So from the above shape we have 3D tensor.\n"," **SHAPE(BATCH SIZE,TOKENS,HIDDEN UNITS)**\n","\n",">  **1** represents one sentence\n","\n",">  **14** represents number of total tokens after tokenization\n","\n",">  **768** the number of hidden units in the **Bert model Uncased**.\n","\n"]},{"cell_type":"code","metadata":{"id":"gofsKTVpj1wE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231131795,"user_tz":-480,"elapsed":14837,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"1ac8eb59-5615-4a72-a834-2925ce0a1258"},"source":["print(len(indexed_tokens), model.config.hidden_size)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["14 768\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"362pUiQheC2U","executionInfo":{"status":"ok","timestamp":1618231131795,"user_tz":-480,"elapsed":14326,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}}},"source":["# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n","assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"yemSMWgDfbtY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231134716,"user_tz":-480,"elapsed":16409,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"fd7de28a-a569-441e-a4c1-9c6ce2955bbd"},"source":["# Load pre-trained model (weights)\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","model.eval()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForMaskedLM(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (cls): BertOnlyMLMHead(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"6lFBXOWvfs1X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231134717,"user_tz":-480,"elapsed":13302,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"240f244d-d015-45df-a34c-40ed3dd1ce59"},"source":["# If you have a GPU, put everything on cuda\n","tokens_tensor = tokens_tensor.to('cuda')\n","segments_tensors = segments_tensors.to('cuda')\n","model.to('cuda')\n"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForMaskedLM(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (cls): BertOnlyMLMHead(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"WsIoJsv5gOZ3","executionInfo":{"status":"ok","timestamp":1618231134718,"user_tz":-480,"elapsed":12791,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}}},"source":["# Predict all tokens\n","with torch.no_grad():\n","    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","    predictions = outputs[0]\n"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"hbvpC5tOgVIX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231135372,"user_tz":-480,"elapsed":13025,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"6ccc3362-6375-4555-8a26-67ef5fa9e354"},"source":["predictions"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ -7.8798,  -7.7874,  -7.7861,  ...,  -7.0438,  -6.7454,  -4.6013],\n","         [-13.3633, -13.7694, -13.7819,  ..., -11.8128, -11.1635, -13.8906],\n","         [-10.9775, -10.5383, -10.9659,  ..., -11.5549,  -8.0309,  -6.3979],\n","         ...,\n","         [ -5.2284,  -5.6572,  -5.3550,  ...,  -3.4507,  -3.8718,  -8.6904],\n","         [ -8.5290,  -8.4146,  -9.0744,  ...,  -7.1710,  -6.9877,  -6.1301],\n","         [-12.5968, -12.3769, -12.4222,  ..., -10.1020,  -9.8764,  -9.4495]]],\n","       device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"-fEdAPEAgRYV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231135373,"user_tz":-480,"elapsed":12374,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}},"outputId":"edb05195-893c-4294-9c1f-2c8ad7e11b3c"},"source":["# confirm we were able to predict 'henson'\n","predicted_index = torch.argmax(predictions[0, masked_index]).item()\n","\n","predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n","print(predicted_index,predicted_token)\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["27227 henson\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7TkRUsuhgkb9","executionInfo":{"status":"ok","timestamp":1618231135374,"user_tz":-480,"elapsed":11846,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}}},"source":["assert predicted_token == 'henson'"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_IlVOysF134","executionInfo":{"status":"ok","timestamp":1618231135374,"user_tz":-480,"elapsed":11416,"user":{"displayName":"陳冠廷","photoUrl":"","userId":"01583028518630001925"}}},"source":[""],"execution_count":21,"outputs":[]}]}