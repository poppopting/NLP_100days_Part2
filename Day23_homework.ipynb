{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Day23_homework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCIvz30AOj-H"
      },
      "source": [
        "# 作業 : 觀察機器翻譯 ATTENTION 內容 \n",
        "- 仔細地觀察機器翻譯 ATTENTION 結果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usP1_X7qOv6F"
      },
      "source": [
        "# [作業目標]\n",
        "- 透過視覺化 注意力 attention 層 了解attention 的作用方式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWGLeN9BOxEF"
      },
      "source": [
        "# [作業重點]\n",
        "- 透過視覺化 注意力 attention 層 了解attention 的作用方式\n",
        "- 原則上只要之前的訓練有跑完，這邊的程式可以執行成功最後只要觀察結果就好\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIBD2Nn-OI-1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import csv\n",
        "import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQoAR8K-RyHd",
        "outputId": "d9db5979-8f46-4bad-c520-871bee051dac"
      },
      "source": [
        "# Colab 進行matplotlib繪圖時顯示繁體中文\n",
        "# 下載字體並命名taipei_sans_tc_beta.ttf，移至指定路徑\n",
        "!wget -O taipei_sans_tc_beta.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n",
        "!mv taipei_sans_tc_beta.ttf /usr/local/lib/python3.6/dist-packages/matplotlib//mpl-data/fonts/ttf\n",
        "\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import matplotlib.pyplot as plt \n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "import matplotlib.ticker as ticker\n",
        "# 自定義字體變數\n",
        "myfont = FontProperties(fname=r'/usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data/fonts/ttf/taipei_sans_tc_beta.ttf')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-07 13:45:52--  https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.142.100, 74.125.142.102, 74.125.142.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.142.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/eji43rkjjtc7s8odo578d4vkvnr5a7j5/1612705500000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_ [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-07 13:45:53--  https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/eji43rkjjtc7s8odo578d4vkvnr5a7j5/1612705500000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
            "Resolving doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-font-ttf]\n",
            "Saving to: ‘taipei_sans_tc_beta.ttf’\n",
            "\n",
            "taipei_sans_tc_beta     [  <=>               ]  19.70M  73.1MB/s    in 0.3s    \n",
            "\n",
            "2021-02-07 13:45:55 (73.1 MB/s) - ‘taipei_sans_tc_beta.ttf’ saved [20659344]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrbNQhl5R2MP",
        "outputId": "9ed0567a-eef4-4cc6-d485-e43c434a1f8e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFjVGqegR5U8",
        "outputId": "89ab12d5-72c1-471f-e950-c458ef78d813"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/NLP_100days_Part2/'\n",
        "lines = open(data_dir + 'cmn.txt' , encoding='utf-8').read().strip().split('\\n')\n",
        "trnslt_pairs = [[s for s in l.split('\\t')] for l in lines ]\n",
        "print (\"Sample: \" , trnslt_pairs[1000][0:2] )\n",
        "print (\"Total records:\" , len(trnslt_pairs))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample:  ['He was drowned.', '他被淹死了。']\n",
            "Total records: 24360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl-KIM-nSA-H",
        "outputId": "d1b35c50-ffb4-4772-a77d-fc6be2fd139d"
      },
      "source": [
        "# 下載 spacy 的英文模型 幫我們做tokenize\n",
        "model_dir =  '/content/drive/MyDrive/NLP_100days_Part2/attention_model/'\n",
        "\n",
        "spacy_eng = spacy.load('en_core_web_sm')\n",
        "def tokenize_eng(text):\n",
        "  #清除不需要的字符\n",
        "  text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
        "  return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "TRG = Field(tokenize = tokenize_eng, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_cmn(text):\n",
        "  #去掉非中文字元\n",
        "  regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n",
        "  text = regex.sub(' ', text)\n",
        "\n",
        "  return [word for word in text if word.strip()]\n",
        "    \n",
        "\n",
        "SRC = Field(tokenize = tokenize_cmn, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            include_lengths = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dataset, dev_dataset, test_dataset = TabularDataset.splits(\n",
        "    path = data_dir , format = 'csv', skip_header = True,\n",
        "    train='train.csv', validation='val.csv', test='test.csv',\n",
        "    fields=[\n",
        "        ('trg', TRG),\n",
        "        ('src', SRC)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# 讀取之前儲存的 vocabulary\n",
        "SRC.build_vocab(train_dataset, min_freq = 1)\n",
        "TRG.build_vocab(train_dataset, min_freq = 1)\n",
        "# SRC.vocab = torch.load(model_dir + 'SRC_vocab.pt')\n",
        "# TRG.vocab = torch.load(model_dir + 'TRG_vocab.pt')\n",
        "\n",
        "print (\"中文語料的字元表長度: \" , len(SRC.vocab) , \", 英文的字元表長度: \" ,len(TRG.vocab))\n",
        "print (\"Sample SRC:\", test_dataset[0].src , \"TRG:\", test_dataset[0].trg)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dataset, dev_dataset, test_dataset), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.src),\n",
        "     device = device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "中文語料的字元表長度:  2699 , 英文的字元表長度:  4097\n",
            "Sample SRC: ['她', '跟', '他', '去', '公', '園', '了'] TRG: ['she', 'went', 'to', 'the', 'park', 'with', 'him', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYoqlKcrq2Z_"
      },
      "source": [
        "# 模型主體 和前面範例程式一樣\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj3ZTHDMSGOF"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs, mask):\n",
        "    # hidden bz , dec_hid_dim\n",
        "    # encoder_outputs src len, bz , enc_hid_dim x 2\n",
        "    # mask bz , src len\n",
        "    \n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "\n",
        "    hidden = hidden.unsqueeze(1) \n",
        "    # hidden unsqueeze bz , 1 , dec_hid_dim\n",
        "\n",
        "    attention = torch.matmul( hidden , encoder_outputs.permute(1, 2, 0)   )\n",
        "    # attention bz, 1 , src len\n",
        "    \n",
        "    attention = attention.squeeze(1)\n",
        "    # squeeze bz , src len\n",
        "\n",
        "    attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "    return F.softmax(attention, dim = 1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNzgZqHcS2CX"
      },
      "source": [
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        # 雙向 ＧＲＵ encoder \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_len):\n",
        "        \n",
        "        #src shape [src len, batch size]\n",
        "        #src_len shape [batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded shape [src len, batch size, emb dim]\n",
        "                \n",
        "        # 使用pack_padded_sequence 來壓縮序列        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
        "                \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "\n",
        "        # 使用 pad_packed_sequence 用來展開序列成原本形狀的      \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "            \n",
        "            \n",
        "        #outputs shape [src len, batch size, hid dim * num directions]\n",
        "        #hidden shape [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden 堆疊 [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs 是最後一層 \n",
        "        \n",
        "        #hidden [-2, :, : ] 是最後一層 forwards RNN \n",
        "        #hidden [-1, :, : ] 是最後一層 backwards RNN\n",
        "        \n",
        "        # hidden 是最後再過一層 dense layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs shape [src len, batch size, enc hid dim * 2]\n",
        "        #hidden shape [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        # 單向 ＧＲＵ decoder \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "             \n",
        "        #input shape [batch size]\n",
        "        #hidden shape [batch size, dec hid dim]\n",
        "        #encoder_outputs shape [src len, batch size, enc hid dim * 2]\n",
        "        #mask shape [batch size, src len]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input shape [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded shape [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "                \n",
        "        #a shape [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a shape [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs shape [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted shape [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted shape [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input shape [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output shape [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden shape [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output shape [1, batch size, dec hid dim]\n",
        "        #hidden shape [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction shape [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
        "\n",
        "class Seq2SeqATTN(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "                    \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)\n",
        "\n",
        "        #mask = [batch size, src len]\n",
        "                \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
        "            #  and mask\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW2KIxhxrMGf"
      },
      "source": [
        "# 建立模型和重要參數 請保持和前面訓練時一樣"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybIY0kKGS_gI",
        "outputId": "39dc2689-c0e8-4613-b8f1-4ac18d162953"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 256 # 注意 encoder hidden layer 設定 必須為 dec 的一半 \n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "LEARNING_RATE = 0.002\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = RNNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = RNNDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "model = Seq2SeqATTN(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "\n",
        "def initial_mdl_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(initial_mdl_weights)\n",
        "print (\"模型全部參數量: {:10,d} \".format(sum(p.numel() for p in model.parameters())))\n",
        "model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "模型全部參數量: 10,009,345 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqATTN(\n",
              "  (encoder): RNNEncoder(\n",
              "    (embedding): Embedding(2699, 256)\n",
              "    (rnn): GRU(256, 256, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): RNNDecoder(\n",
              "    (attention): Attention()\n",
              "    (embedding): Embedding(4097, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=4097, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOpjxQJmTDYU"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, src_len = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, src_len.cpu(), trg, 0) #turn off teacher forcing\n",
        "            \n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukg9t_iOTHlG",
        "outputId": "86efbcfb-f516-4def-b7c0-e5e25f2ba6fc"
      },
      "source": [
        "model_dir =  '/content/drive/MyDrive/NLP_100days_Part2/attention_model/'\n",
        "model.load_state_dict(torch.load(model_dir + 'best-model.pt'))\n",
        "#model.load_state_dict(torch.load(model_dir + 'model-7.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.607 | Test PPL:  36.865 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-caE1Y1TL5p"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    #if isinstance(sentence, str):\n",
        "    #    nlp = spacy_en = spacy.load('en_core_web_sm')\n",
        "    #    tokens = [token.text.lower() for token in spacy_en(sentence)]\n",
        "    #else:\n",
        "    #    tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [token.lower() for token in sentence]\n",
        "        \n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor, src_len.cpu())\n",
        "\n",
        "    mask = model.create_mask(src_tensor)\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "        attentions[i] = attention\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jhh_5_SYYLT"
      },
      "source": [
        "def display_attention(sentence, translation, attention):\n",
        "    \n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    \n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "   \n",
        "    #fontdict = {\"fontproperties\": zhfont}\n",
        "    \n",
        "    #ax.set_xticks(range(max(max_len_tar, len(predicted_seq))))\n",
        "    #ax.set_xlim(-0.5, max_len_tar -1.5)\n",
        "    \n",
        "    #ax.set_yticks(range(len(sentence) + 2))\n",
        "    #ax.set_xticklabels([subword_encoder_zh.decode([i]) for i in predicted_seq \n",
        "    #                    if i < subword_encoder_zh.vocab_size], \n",
        "    #                   fontdict=fontdict, fontsize=18)\n",
        "    \n",
        "    #plt.rcParams[\"font.family\"]=\"sans-serif\"\n",
        "    #plt.rcParams['font.sans-serif']=['STSong'] #用来正常显示中文标签\n",
        "    \n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                       rotation=45 , fontproperties=myfont) #, fontdict=fontdict)\n",
        "    ax.set_yticklabels(['']+translation, fontproperties=myfont) # , fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4n7915Mrcs1"
      },
      "source": [
        "# 作業重點\n",
        "## 請選擇一個好的翻譯結果\n",
        "## 將其 ATTENTION 視覺化 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRYXjqgvYb-E",
        "outputId": "08b686f9-f1d0-4608-8cf6-727d16ff1ad5"
      },
      "source": [
        "# 請在這邊自行調整 sample index \n",
        "# 觀察不同句子的 ATTENTION 結果\n",
        "example_idx =499\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['src']\n",
        "trg = vars(train_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['我', '晚', '飯', '後', '彈', '吉', '他']\n",
            "trg = ['i', 'play', 'the', 'guitar', 'after', 'dinner', '.']\n",
            "predicted trg = ['i', 'play', 'the', 'guitar', 'after', 'dinner', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cdlptGJrsfv"
      },
      "source": [
        "# 請觀察翻譯文 和被翻譯文的語意對應"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "H8f8csPSYfkU",
        "outputId": "8d575bd3-ecf9-4e8d-fcea-bc02358e3a84"
      },
      "source": [
        "print (\"\".join(src ))\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "我晚飯後彈吉他\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}