{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>主題:\n",
    "啤酒評論評分預測 - 分詞器(Tokenizer)\n",
    "### <strong>說明:\n",
    "繼續上次啤酒的評鑑資料集的練習，還記得在上次的資料初步分析之後，我們對於BERT模型最大 <br />\n",
    "長度的限制是255嗎? 然而這還只是未使用分詞器所得到的結論，當真正使用分詞器之後，每個 <br />\n",
    "評論具的token數可能會與我們先前的評估有所不同，因此這是作業主要是以BERT分瓷器來驗證 <br />\n",
    "上次作業得到的結論是否需要修正。\n",
    "### <strong>題目\n",
    "1. 創建英文BERT所使用的分詞器，提供下一題分析以及後續訓練使用\n",
    "2. 以分詞器生成每個評論語句的token，並評估上次得到的最大長度限制是否合理\n",
    "\n",
    "#### <strong>提示: 不要忘記加上[SEP]與[CLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = pd.read_json(\"train_set.json\")\n",
    "TEST = pd.read_json(\"test_set.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "TOKENIZER = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pours a clouded gold with a thin white head. Nose is quite floral with a larger amount of spices added. Definitely a spice forward fragrance. Flavor has an odd burn that hits on the first sip. After it fades it seems like a dirty vanilla aftertaste. Perhaps this is the absinthe? Regardless of that, I get a quite spiced tone on the tongue. Almost feel a little heat from it. I think that my inexperienced palate on these spices is contributing to my ignorance of what precisely they are. Overall a nice drinker indeed.\n",
      "['Po', '##urs', 'a', 'cloud', '##ed', 'gold', 'with', 'a', 'thin', 'white', 'head', '.', 'No', '##se', 'is', 'quite', 'floral', 'with', 'a', 'larger', 'amount', 'of', 'spices', 'added', '.', 'De', '##fin', '##ite', '##ly', 'a', 's', '##pice', 'forward', 'f', '##rag', '##rance', '.', 'F', '##lav', '##or', 'has', 'an', 'odd', 'burn', 'that', 'hits', 'on', 'the', 'first', 'sip']\n",
      "[18959, 7719, 170, 7180, 1174, 2284, 1114, 170, 4240, 1653, 1246, 119, 1302, 2217, 1110, 2385, 22504, 1114, 170, 2610, 2971, 1104, 25133, 1896, 119, 3177, 16598, 3150, 1193, 170, 188, 15633, 1977, 175, 20484, 10555, 119, 143, 9516, 1766, 1144, 1126, 5849, 6790, 1115, 4919, 1113, 1103, 1148, 11456]\n"
     ]
    }
   ],
   "source": [
    "text = TRAIN['review/text'][0]\n",
    "tokens = TOKENIZER.tokenize(text)\n",
    "tokens_id = TOKENIZER.convert_tokens_to_ids(tokens)\n",
    "print(text)\n",
    "print(tokens[:50])\n",
    "print(tokens_id[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP] 102\n",
      "[CLS] 101\n",
      "[PAD] 0\n",
      "[UNK] 100\n"
     ]
    }
   ],
   "source": [
    "print(TOKENIZER.sep_token, TOKENIZER.sep_token_id)\n",
    "print(TOKENIZER.cls_token, TOKENIZER.cls_token_id)\n",
    "print(TOKENIZER.pad_token, TOKENIZER.pad_token_id)\n",
    "print(TOKENIZER.unk_token, TOKENIZER.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101, 18959,  7719,   170,  7180,  1174,  2284,  1114,   170,  4240,\n",
      "         1653,  1246,   119,  1302,  2217,  1110,  2385, 22504,  1114,   170,\n",
      "         2610,  2971,  1104, 25133,  1896,   119,  3177, 16598,  3150,  1193,\n",
      "          170,   188, 15633,  1977,   175, 20484, 10555,   119,   143,  9516,\n",
      "         1766,  1144,  1126,  5849,  6790,  1115,  4919,  1113,  1103,  1148,\n",
      "        11456,   119,  1258,  1122, 15854,  1116,  1122,  3093,  1176,   170,\n",
      "         7320,  3498,  5878,  1170, 10401,  1566,   119,  5203,  1142,  1110,\n",
      "         1103,   170,  4832, 10879,  4638,   136, 20498,  1104,  1115,   117,\n",
      "          146,  1243,   170,  2385,   188, 15633,  1181,  3586,  1113,  1103,\n",
      "         3661,   119,  8774,  1631,   170,  1376,  3208,  1121,  1122,   119,\n",
      "          146,  1341,  1115,  1139,  1107, 11708,  3365, 28118,   185,  5971,\n",
      "         1566,  1113,  1292, 25133,  1110,  7773,  1106,  1139, 21326,  1104,\n",
      "         1184, 11228,  1152,  1132,   119,  8007,   170,  3505,  3668,  1200,\n",
      "         5750,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guan-Ting Chen\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "encoding = TOKENIZER.encode_plus(\n",
    "  text,\n",
    "  max_length=MAX_SEQ_LEN,\n",
    "  add_special_tokens=True,\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  truncation=True,\n",
    "  return_tensors='pt',\n",
    ")\n",
    "\n",
    "print(encoding[\"input_ids\"][0])\n",
    "print(encoding[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Po', '##urs', 'a', 'cloud', '##ed', 'gold', 'with', 'a', 'thin', 'white', 'head', '.', 'No', '##se', 'is', 'quite', 'floral', 'with', 'a', 'larger', 'amount', 'of', 'spices', 'added', '.', 'De', '##fin', '##ite', '##ly', 'a', 's', '##pice', 'forward', 'f', '##rag', '##rance', '.', 'F', '##lav', '##or', 'has', 'an', 'odd', 'burn', 'that', 'hits', 'on', 'the', 'first', 'sip', '.', 'After', 'it', 'fade', '##s', 'it', 'seems', 'like', 'a', 'dirty', 'van', '##illa', 'after', '##tas', '##te', '.', 'Perhaps', 'this', 'is', 'the', 'a', '##bs', '##int', '##he', '?', 'Regardless', 'of', 'that', ',', 'I', 'get', 'a', 'quite', 's', '##pice', '##d', 'tone', 'on', 'the', 'tongue', '.', 'Almost', 'feel', 'a', 'little', 'heat', 'from', 'it', '.', 'I', 'think', 'that', 'my', 'in', '##ex', '##per', '##ienced', 'p', '##ala', '##te', 'on', 'these', 'spices', 'is', 'contributing', 'to', 'my', 'ignorance', 'of', 'what', 'precisely', 'they', 'are', '.', 'Overall', 'a', 'nice', 'drink', '##er', 'indeed', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(TOKENIZER.convert_ids_to_tokens(encoding[\"input_ids\"][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
